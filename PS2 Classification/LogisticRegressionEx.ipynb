{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181fe757",
   "metadata": {},
   "source": [
    "##  Programming Exercise: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e954c",
   "metadata": {},
   "source": [
    "In this exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. \n",
    "\n",
    "Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision. Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ac360",
   "metadata": {},
   "source": [
    "### 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6448980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('exams_data.txt', header=None)\n",
    "df.columns = ['exam_score_1', 'exam_score_2', 'admitted']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some useful values\n",
    "\n",
    "m = df.shape[0] # number of training samples\n",
    "X = np.hstack((np.ones((m, 1)), df[['exam_score_1', 'exam_score_2']].values))\n",
    "y = np.array(df['admitted'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e6c87",
   "metadata": {},
   "source": [
    "### 2. Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bf816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    \"\"\" Plots the data points X and y with + for the positive examples and . for the negative examples. \n",
    "        X is assumed to be a mx3 matrix.\n",
    "    \"\"\"\n",
    "  \n",
    "    # ====================== YOUR CODE HERE ====================================\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a3f3b8",
   "metadata": {},
   "source": [
    "### 3. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9aa48",
   "metadata": {},
   "source": [
    "Logistic regression hypothesis: \n",
    "\n",
    "$$h_\\theta(x) = g(\\theta^Tx)$$\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20dda3",
   "metadata": {},
   "source": [
    "#### 3.1 Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13be4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" g = sigmoid(z) computes the sigmoid of z (z can be a scalar, vector or a matrix).\n",
    "    \"\"\"\n",
    "  \n",
    "    # ====================== YOUR CODE HERE =======================\n",
    "    \n",
    "    \n",
    "    # ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid([0, 0.1, 0.5, 0.9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4c28c",
   "metadata": {},
   "source": [
    "#### 3.2 Cost function and gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2acc6f0",
   "metadata": {},
   "source": [
    "Cost function in logistic regression is:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m[y^i log(h_\\theta(x^i))+(1-y^i)log(1-h_\\theta(x^i))]$$\n",
    "\n",
    "Vectorized implementation:\n",
    "\n",
    "$h = g(X\\theta)$\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m}(-y^T log(h)-(1-y)^Tlog(1-h))$\n",
    "\n",
    "\n",
    "\n",
    "The gradient of the cost is a vector of the same length as $\\theta$ where $j^{th}$ element (for $j=0,1,...,n$) is defined as follows:\n",
    "\n",
    "$$\\nabla J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m ((h_\\theta(x^i) - y^i) \\cdot x_j^i)$$\n",
    "\n",
    "Vectorized:\n",
    "$\\nabla J(\\theta) = \\frac{1}{m} \\cdot X^T \\cdot (g(X\\theta)-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost and gradient for logistic regression\n",
    "def cost_function(theta, X, y):\n",
    "    \"\"\" J, grad = cost_function(theta, X, y) computes the cost of using theta as the parameter \n",
    "        for logistic regression and the gradient of the cost w.r.t. to the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(len(theta))\n",
    "  \n",
    "    # ====================== YOUR CODE HERE ====================================\n",
    "    # Instructions: Compute the cost of a particular choice of theta.\n",
    "    #               You should set J to the cost.\n",
    "    #               Compute the partial derivatives and set grad to the partial\n",
    "    #               derivatives of the cost w.r.t. each parameter in theta\n",
    "    #\n",
    "    # Note: grad should have the same dimensions as theta\n",
    "    #\n",
    "    # DIMENSIONS: \n",
    "    #   theta = (n+1) x 1\n",
    "    #   X     = m x (n+1)\n",
    "    #   y     = m x 1\n",
    "    #   grad  = (n+1) x 1\n",
    "    #   J     = Scalar\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37670083",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(shape=(X.shape[1]))\n",
    "cost, grad = cost_function(initial_theta, X, y)\n",
    "\n",
    "print('Cost at initial theta (zeros):', cost)\n",
    "print('Expected cost (approx): 0.693')\n",
    "print('Gradient at initial theta (zeros):')\n",
    "print(grad)\n",
    "print('Expected gradients (approx):\\n -0.1000\\n -12.0092\\n -11.2628')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ebddd",
   "metadata": {},
   "source": [
    "#### 3.3 Learning parameters using an optimization solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2d481",
   "metadata": {},
   "source": [
    "\"Conjugate gradient\", \"BFGS\", and \"TNC (Truncated Newton)\" are more sophisticated, faster ways to optimize \n",
    " that can be used instead of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "def optimize_theta(X, y, initial_theta):\n",
    "    opt_results = opt.minimize(cost_function, initial_theta, args=(X, y), method='TNC',\n",
    "                               jac=True, options={'maxiter':400})\n",
    "    return opt_results['x'], opt_results['fun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_theta, cost = optimize_theta(X, y, initial_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc827acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cost at theta found by fminunc:', cost)\n",
    "print('Expected cost (approx): 0.203')\n",
    "print('theta:\\n', opt_theta.reshape(-1,1))\n",
    "print('Expected theta (approx):')\n",
    "print(' -25.161\\n 0.206\\n 0.201')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f8514",
   "metadata": {},
   "source": [
    "### 4. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = sigmoid(np.array([1, 45, 85]).dot(opt_theta))\n",
    "print('For a student with scores 45 and 85, we predict an admission probability of', prob)\n",
    "print('Expected value: 0.775 +/- 0.002')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6295fd",
   "metadata": {},
   "source": [
    "#### 4.1 Accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ff435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    \"\"\" Predict whether the label is 0 or 1 using learned logistic regression parameters theta\n",
    "        y_pred = PREDICT(theta, X) computes the predictions for X using a threshold at 0.5 \n",
    "        (i.e., if sigmoid(X @ theta) >= 0.5, predict 1)\n",
    "    \"\"\"\n",
    "  \n",
    "    # You need to return the following variables correctly\n",
    "    y_pred = np.zeros(m)\n",
    "  \n",
    "    # ====================== YOUR CODE HERE ===================================\n",
    "    # Instructions: Complete the following code to make predictions using\n",
    "    #               your learned logistic regression parameters. \n",
    "    #               You should set p to a vector of 0's and 1's\n",
    "    #\n",
    "    # Dimentions:\n",
    "    # X     =  m x (n+1)\n",
    "    # theta = (n+1) x 1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(opt_theta, X)\n",
    "print(f'Train accuracy: {np.mean(y_pred == y) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6612b",
   "metadata": {},
   "source": [
    "#### 4.2 Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_with_decision_boundary(theta, X, y):\n",
    "    \"\"\" Plots the training data with the decision boundary\n",
    "    \"\"\"\n",
    "  \n",
    "    # ====================== YOUR CODE HERE ===================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14337424",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_with_decision_boundary(opt_theta, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
